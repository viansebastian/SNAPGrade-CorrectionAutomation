{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO \n",
    "\n",
    "def resize_image(image, target_size=(800, 1000)):\n",
    "  '''Resize the image while maintaining aspect ratio'''\n",
    "  h, w = image.shape[:2]\n",
    "  scale = min(target_size[1] / w, target_size[0] / h)\n",
    "  new_w = int(w * scale)\n",
    "  new_h = int(h * scale)\n",
    "  resized_image = cv2.resize(image, (new_w, new_h))\n",
    "  return resized_image\n",
    "\n",
    "def logarithmic_transformation(image, epsilon=1e-5):\n",
    "  '''Apply logarithmic transformation to the image with zero value handling'''\n",
    "  c = 255 / np.log(1 + np.max(image))\n",
    "  # Epsilon zero-handling technique\n",
    "  log_image = c * (np.log(1 + image + epsilon))\n",
    "  log_image = np.array(log_image, dtype=np.uint8)\n",
    "\n",
    "  return log_image\n",
    "\n",
    "def contrast_stretching(image):\n",
    "  min_val = np.min(image)\n",
    "  max_val = np.max(image)\n",
    "  stretched = (image - min_val) * (255 / (max_val - min_val))\n",
    "  return stretched.astype(np.uint8)\n",
    "\n",
    "def gaussian_blur(image, mode='Soft'):\n",
    "  if mode == 'Soft':\n",
    "    kernel_size = (3,3)\n",
    "  elif mode == 'Medium':\n",
    "    kernel_size = (5,5)\n",
    "  elif mode == 'Hard':\n",
    "    kernel_size = (7,7)\n",
    "  else:\n",
    "    raise ValueError(\"Mode must be 'Soft', 'Medium', or 'Hard'\")\n",
    "\n",
    "  return cv2.GaussianBlur(image, kernel_size, 0)\n",
    "\n",
    "def measure_blurriness(image):\n",
    "  # Apply the Laplacian operator to detect edges\n",
    "  laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "  # Variance of Laplacian\n",
    "  variance = laplacian.var()\n",
    "\n",
    "  return variance\n",
    "\n",
    "def adaptive_gaussian_blur(image, desired_blur=100, max_iterations=100):\n",
    "  # Measure initial blur level\n",
    "  initial_blur = measure_blurriness(image)\n",
    "\n",
    "  # Set a starting kernel size\n",
    "  kernel_size = 5\n",
    "\n",
    "  for iteration in range(max_iterations):\n",
    "      # Apply Gaussian blur with the current kernel size\n",
    "      blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "\n",
    "      # Measure the blur after applying Gaussian blur\n",
    "      current_blur = measure_blurriness(blurred_image)\n",
    "\n",
    "      # If the current blur exceeds the desired blur, stop\n",
    "      if current_blur > desired_blur:\n",
    "          kernel_size += 2\n",
    "      else:\n",
    "        break\n",
    "\n",
    "  final_blurred_img = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "  final_blur = measure_blurriness(final_blurred_img)\n",
    "\n",
    "  print(f\"Initial Blur: {initial_blur}, Final Blur: {final_blur}, Kernel Size: {kernel_size}, Iterations: {iteration+1}\")\n",
    "\n",
    "  return final_blurred_img\n",
    "\n",
    "def clahe_equalization(image):\n",
    "  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "  equalized_img = clahe.apply(image)\n",
    "  return equalized_img\n",
    "\n",
    "def otsu_thresholding(image):\n",
    "  _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "  return binary_image\n",
    "\n",
    "def canny_edge_detection(image, low_threshold=50, high_threshold=150):\n",
    "  return cv2.Canny(image, low_threshold, high_threshold)\n",
    "\n",
    "def find_extreme_corners(contours):\n",
    "  '''Find the extreme corners of the image'''\n",
    "  all_points = np.vstack(contours)\n",
    "  top_left = all_points[np.argmin(all_points[:, :, 0] + all_points[:, :, 1])]\n",
    "  bottom_right = all_points[np.argmax(all_points[:, :, 0] + all_points[:, :, 1])]\n",
    "  top_right = all_points[np.argmax(all_points[:, :, 0] - all_points[:, :, 1])]\n",
    "  bottom_left = all_points[np.argmin(all_points[:, :, 0] - all_points[:, :, 1])]\n",
    "  return top_left[0], top_right[0], bottom_left[0], bottom_right[0]\n",
    "\n",
    "def apply_perspective_transformation(image, corners):\n",
    "  '''Apply perspective transformation to the image'''\n",
    "  tl, tr, bl, br = corners\n",
    "  width = int(max(np.linalg.norm(br - bl), np.linalg.norm(tr - tl)))\n",
    "  height = int(max(np.linalg.norm(tr - br), np.linalg.norm(tl - bl)))\n",
    "\n",
    "  dst_pts = np.array([\n",
    "      [0, 0],\n",
    "      [width - 1, 0],\n",
    "      [0, height - 1],\n",
    "      [width - 1, height - 1]\n",
    "  ], dtype=\"float32\")\n",
    "\n",
    "  src_pts = np.array([tl, tr, bl, br], dtype=\"float32\")\n",
    "\n",
    "  M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "  warped = cv2.warpPerspective(image, M, (width, height))\n",
    "  return warped\n",
    "\n",
    "def automatic_warp_transformation(image, target_size=(800, 1000)):\n",
    "  '''Automatic Cropping using Adaptive Warp Transformation'''\n",
    "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  resized_image = resize_image(gray_image, target_size)\n",
    "  brightened_image = logarithmic_transformation(resized_image)\n",
    "  contrast_image = contrast_stretching(brightened_image)\n",
    "  blurred_image = gaussian_blur(contrast_image, mode='Soft')\n",
    "  binary_image = otsu_thresholding(blurred_image)\n",
    "  edges = canny_edge_detection(binary_image)\n",
    "  contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  # Getting Contours (Drawing Contours in image, useful for debugging)\n",
    "  contour_image = cv2.cvtColor(binary_image, cv2.COLOR_GRAY2BGR)\n",
    "  cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "  corners = find_extreme_corners(contours)\n",
    "  for corner in corners:\n",
    "      cv2.circle(contour_image, tuple(corner), 5, (0, 0, 255), -1)\n",
    "\n",
    "  warped_image = apply_perspective_transformation(resized_image, corners)\n",
    "  print(f'Initial image {image.shape} processed to {warped_image.shape}')\n",
    "\n",
    "  return warped_image\n",
    "\n",
    "def automatic_warp_transformation_v2(image, target_size=(800, 1000)):\n",
    "  '''Automatic Cropping using Adaptive Warp Transformation'''\n",
    "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  resized_image = resize_image(gray_image, target_size)\n",
    "  \n",
    "  clahe = clahe_equalization(resized_image)\n",
    "  log_img = logarithmic_transformation(clahe)\n",
    "  contrast_img = contrast_stretching(log_img)\n",
    "  blurred_img = gaussian_blur(contrast_img)\n",
    "  binary_img = otsu_thresholding(blurred_img)\n",
    "  edges = canny_edge_detection(binary_img)\n",
    "  contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  # Getting Contours (Drawing Contours in image, useful for debugging)\n",
    "  contour_image = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2BGR)\n",
    "  cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "  corners = find_extreme_corners(contours)\n",
    "  for corner in corners:\n",
    "      cv2.circle(contour_image, tuple(corner), 5, (0, 0, 255), -1)\n",
    "\n",
    "  warped_image = apply_perspective_transformation(resized_image, corners)\n",
    "  print(f'Initial image {image.shape} processed to {warped_image.shape}')\n",
    "\n",
    "  return warped_image\n",
    "\n",
    "def image_uniformization(master_image, student_image):\n",
    "  '''Precision Image Resizing'''\n",
    "  master_shape = master_image.shape\n",
    "  student_shape = student_image.shape\n",
    "\n",
    "  master_height = master_shape[0]\n",
    "  master_width = master_shape[1]\n",
    "\n",
    "  student_height = student_shape[0]\n",
    "  student_width = student_shape[1]\n",
    "\n",
    "  min_height = min(master_height, student_height)\n",
    "  min_width = min(master_width, student_width)\n",
    "\n",
    "  resized_master = cv2.resize(master_image, (min_width, min_height))\n",
    "  resized_student = cv2.resize(student_image, (min_width, min_height))\n",
    "\n",
    "  print(f'master_key {master_image.shape} and student_answer {student_image.shape} uniformed to {resized_master.shape}')\n",
    "\n",
    "  return resized_master, resized_student\n",
    "\n",
    "def morph_open(image):\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "  eroded_img = cv2.erode(image, kernel, iterations = 1)\n",
    "  dilated_img = cv2.dilate(eroded_img, kernel, iterations = 1)\n",
    "\n",
    "  return dilated_img\n",
    "\n",
    "def core_preprocessing(image):\n",
    "  '''Core Preprocessing Module'''\n",
    "  blurred_img = gaussian_blur(image, mode='Hard')\n",
    "  contrast_img = contrast_stretching(blurred_img)\n",
    "  log_img = logarithmic_transformation(contrast_img)\n",
    "  binary_img = otsu_thresholding(log_img)\n",
    "  opened_img = morph_open(binary_img)\n",
    "\n",
    "  return opened_img\n",
    "\n",
    "def core_preprocessing_v2(image):\n",
    "  '''\n",
    "  Core Preprocessing Module V2:\n",
    "  - Uses CLAHE for lighting handling\n",
    "  - Uses Adaptive Gaussian Blur to ensure optimal thresholding\n",
    "  '''\n",
    "  clahe_img = clahe_equalization(image)\n",
    "  blurred_img = adaptive_gaussian_blur(clahe_img, desired_blur=100, max_iterations=100)\n",
    "  contrast_img = contrast_stretching(blurred_img)\n",
    "  log_img = logarithmic_transformation(contrast_img)\n",
    "  binary_img = otsu_thresholding(log_img)\n",
    "  opened_img = morph_open(binary_img)\n",
    "\n",
    "  return opened_img\n",
    "\n",
    "def draw_full_contours(contours, cont_image, radius = 7):\n",
    "  '''Draw Full Circles'''\n",
    "  for contour in contours:\n",
    "    M = cv2.moments(contour)\n",
    "    if M[\"m00\"] != 0:\n",
    "      cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "      cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "      # Draw a filled circle at the center of the contour\n",
    "      cv2.circle(cont_image, (cX, cY), radius, (0, 255, 0), -1)\n",
    "\n",
    "  return cont_image\n",
    "\n",
    "def extract_and_draw_contours(image):\n",
    "  contours, hierarchy = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  unique_values = []\n",
    "  for columns in image:\n",
    "    for pixel in columns:\n",
    "      if pixel not in unique_values:\n",
    "        unique_values.append(pixel)\n",
    "\n",
    "  contour_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "  contour_image = draw_full_contours(contours, contour_image)\n",
    "\n",
    "  return contours, contour_image\n",
    "\n",
    "def extract_and_draw_circle_contours(image):\n",
    "  contours, hierarchy = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  circle_contours = []\n",
    "  contour_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "  # radius_list = []\n",
    "  # contour_ar_list = []\n",
    "\n",
    "  for contour in contours:\n",
    "      # Approximate the enclosing circle for each contour\n",
    "      (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "      circle_area = np.pi * (radius ** 2)\n",
    "\n",
    "      # if radius not in radius_list:\n",
    "      #   radius_list.append(radius)\n",
    "\n",
    "      # if contour_area not in contour_ar_list:\n",
    "      #   contour_ar_list.append(contour_area)\n",
    "\n",
    "      # Calculate the actual contour area\n",
    "      contour_area = cv2.contourArea(contour)\n",
    "\n",
    "      # Check if the contour area is approximately equal to the circle area\n",
    "      # Tolerance range for being \"circular\"\n",
    "      if radius < 5:\n",
    "          if 0.6 <= contour_area / circle_area <= 1.4:\n",
    "              circle_contours.append(contour)\n",
    "      else:\n",
    "          if 0.8 <= contour_area / circle_area <= 1.2:\n",
    "              circle_contours.append(contour)\n",
    "\n",
    "  # contour_image = cv2.drawContours(contour_image, circle_contours, -1, (0, 255, 0), thickness=2)\n",
    "  contour_image = draw_full_contours(circle_contours, contour_image)\n",
    "\n",
    "  return circle_contours, contour_image\n",
    "\n",
    "def soft_morph_open(image):\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "  eroded_img = cv2.erode(image, kernel, iterations = 1)\n",
    "  dilated_img = cv2.dilate(eroded_img, kernel, iterations = 1)\n",
    "\n",
    "  return dilated_img\n",
    "\n",
    "# ----------------------\n",
    "def yolo_catch_image(model, image): \n",
    "    input = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    res = model.predict(input)\n",
    "    pred = res[0]\n",
    "    boxes = pred.boxes  \n",
    "    coords = boxes.xywh  \n",
    "    confidences = boxes.conf  \n",
    "    class_ids = boxes.cls  \n",
    "    \n",
    "    return pred, boxes, coords\n",
    "\n",
    "def get_max_width_height(master_coords, student_coords):\n",
    "    \"\"\"\n",
    "    Computes the maximum width and height from master and student bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        master_coords (torch.Tensor or numpy.ndarray): Bounding box coordinates for the master image (center_x, center_y, width, height).\n",
    "        student_coords (torch.Tensor or numpy.ndarray): Bounding box coordinates for the student image (center_x, center_y, width, height).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Maximum width and maximum height across both sets of coordinates.\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "\n",
    "    # Combine both sets of coordinates\n",
    "    all_coords = torch.cat([master_coords, student_coords], dim=0) if isinstance(master_coords, torch.Tensor) else np.vstack((master_coords, student_coords))\n",
    "    \n",
    "    # Traverse all bounding box dimensions\n",
    "    for coord in all_coords:\n",
    "        # Convert tensor to NumPy array if needed\n",
    "        if isinstance(coord, torch.Tensor):\n",
    "            coord = coord.cpu().numpy()\n",
    "        _, _, width, height = coord.astype(int)\n",
    "        \n",
    "        # Update maximum width and height\n",
    "        max_width = max(max_width, width)\n",
    "        max_height = max(max_height, height)\n",
    "\n",
    "    # return max(max_width, max_height)\n",
    "    return int((max_width + max_height) / 2)\n",
    "\n",
    "def mark_ans_box(image, coords, shape=\"box\", size=10, color=(0, 255, 0), thickness=-1):\n",
    "    \"\"\"\n",
    "    Draw a fixed-size filled box or circle at the center of each detected box.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image where the shapes will be drawn.\n",
    "        boxes (object): The detected boxes (used for validation, if needed).\n",
    "        coords (torch.Tensor or numpy.ndarray): Box coordinates (center x, center y, width, height).\n",
    "        shape (str): The shape to draw (\"box\" or \"circle\"). Default is \"box\".\n",
    "        size (int): The size (side length or radius) of the shape. Default is 10.\n",
    "        color (tuple): The color of the shape in (B, G, R). Default is green.\n",
    "        thickness (int): Thickness of the shape. Use -1 for filled shapes. Default is -1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the shapes drawn on it.\n",
    "    \"\"\"\n",
    "    input_img = image.copy()\n",
    "    # input_img = cv2.cvtColor(input_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    for coord in coords:\n",
    "        # Convert tensor to NumPy array if needed and cast to integers\n",
    "        if isinstance(coord, torch.Tensor):\n",
    "            coord = coord.cpu().numpy()\n",
    "        center_x, center_y, _, _ = coord.astype(int)\n",
    "\n",
    "        if shape == \"box\":\n",
    "            # Calculate the top-left and bottom-right coordinates of the box\n",
    "            top_left = (center_x - size // 2, center_y - size // 2)\n",
    "            bottom_right = (center_x + size // 2, center_y + size // 2)\n",
    "            # Draw the box\n",
    "            cv2.rectangle(input_img, top_left, bottom_right, color, thickness)\n",
    "        elif shape == \"circle\":\n",
    "            # Draw the circle\n",
    "            cv2.circle(input_img, (center_x, center_y), size, color, thickness)\n",
    "        else:\n",
    "            raise ValueError(\"Shape must be either 'box' or 'circle'.\")\n",
    "\n",
    "    return input_img\n",
    "\n",
    "def draw_filled_boxes(contours, cont_image):\n",
    "    \"\"\"\n",
    "    Draws filled boxes inside each white region in a binary image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Grayscale input image with white regions on a black background.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with filled boxes drawn inside each white region.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = cont_image.copy()\n",
    "    input_img = cv2.cvtColor(input_img, cv2.COLOR_GRAY2RGB)\n",
    "    for contour in contours:\n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Calculate the center of the rectangle\n",
    "        center_x, center_y = x + w // 2, y + h // 2\n",
    "        \n",
    "        # Define the size of the filled box\n",
    "        size = min(w, h) // 2  # Adjust box size to fit within the region\n",
    "        \n",
    "        # Draw a filled rectangle centered within the bounding box\n",
    "        top_left = (center_x - size, center_y - size)\n",
    "        bottom_right = (center_x + size, center_y + size)\n",
    "        cv2.rectangle(input_img, top_left, bottom_right, (0, 255, 0), -1)  \n",
    "\n",
    "    return input_img\n",
    "\n",
    "def draw_filled_boxes_fetch_contours(image):\n",
    "    \"\"\"\n",
    "    Draws filled boxes inside each white region in a binary image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Grayscale input image with white regions on a black background.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with filled boxes drawn inside each white region.\n",
    "    \"\"\"\n",
    "    # Convert to binary if not already\n",
    "    # _, binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    input_img = image.copy()\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(input_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Convert to color for drawing\n",
    "    result_image = cv2.cvtColor(input_img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Calculate the center of the rectangle\n",
    "        center_x, center_y = x + w // 2, y + h // 2\n",
    "        \n",
    "        # Define the size of the filled box\n",
    "        size = min(w, h) // 2  # Adjust box size to fit within the region\n",
    "        \n",
    "        # Draw a filled rectangle centered within the bounding box\n",
    "        top_left = (center_x - size, center_y - size)\n",
    "        bottom_right = (center_x + size, center_y + size)\n",
    "        cv2.rectangle(result_image, top_left, bottom_right, (0, 255, 0), -1)  # Green filled box\n",
    "\n",
    "    return contours, result_image\n",
    "\n",
    "def draw_box_fill_space_overlap(master_contours, student_contours, image_shape, overlap_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Draws contours with conditions:\n",
    "    - Regions with >=10% overlap between student and master contours are filled green.\n",
    "    - Non-overlapping or <10% overlap contours from student_contours are left white.\n",
    "\n",
    "    Args:\n",
    "        master_contours (list): Contours from the master (reference) image.\n",
    "        student_contours (list): Contours from the student (target) image.\n",
    "        image_shape (tuple): Shape of the new image (height, width, channels).\n",
    "        overlap_threshold (float): Minimum overlap ratio to classify as overlapping (default: 10%).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with processed contours.\n",
    "    \"\"\"\n",
    "    # Create a blank image for the result\n",
    "    result_img = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Create masks for master and student contours\n",
    "    master_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    # student_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Draw the master contours on the master mask\n",
    "    cv2.drawContours(master_mask, master_contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    for contour in student_contours:\n",
    "        # Create an individual mask for the current student contour\n",
    "        single_student_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "        cv2.drawContours(single_student_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Find overlapping regions using bitwise AND\n",
    "        overlap_mask = cv2.bitwise_and(master_mask, single_student_mask)\n",
    "\n",
    "        # Calculate the overlap ratio\n",
    "        overlap_area = np.sum(overlap_mask > 0)\n",
    "        student_area = np.sum(single_student_mask > 0)\n",
    "        overlap_ratio = overlap_area / student_area if student_area > 0 else 0\n",
    "\n",
    "        # Decide color based on overlap percentage\n",
    "        if overlap_ratio >= overlap_threshold:\n",
    "            # Fill green for significant overlap\n",
    "            result_img[overlap_mask > 0] = (0, 255, 0)\n",
    "        else:\n",
    "            # Fill white for non-overlapping or low-overlap regions\n",
    "            result_img[single_student_mask > 0] = (255, 255, 255)\n",
    "\n",
    "    return result_img\n",
    "\n",
    "\n",
    "def get_cross_answers(model, master_image, student_image): \n",
    "    master_pred, master_box, master_coords = yolo_catch_image(model, master_image)\n",
    "    student_pred, student_box, student_coords = yolo_catch_image(model, student_image)\n",
    "    \n",
    "    box_size = get_max_width_height(master_coords, student_coords)\n",
    "    \n",
    "    master_img = mark_ans_box(master_image, master_coords, shape='box', size=box_size, color=(0, 255, 0), thickness=-1)\n",
    "    student_img = mark_ans_box(student_image, student_coords, shape='box', size=box_size, color=(0, 255, 0), thickness=-1)\n",
    "    \n",
    "    return master_img, student_img\n",
    "\n",
    "def box_contour_handling(master_image, student_image):\n",
    "    master_contours, master_output = draw_filled_boxes_fetch_contours(master_image)\n",
    "    student_contours, student_output = draw_filled_boxes_fetch_contours(student_image)\n",
    "    \n",
    "    shape = (student_image.shape[0], student_image.shape[1], 3)\n",
    "    stu_mistake_loc_cont_img = draw_box_fill_space_overlap(master_contours, student_contours, shape)\n",
    "    stu_mistake_loc_cont_img = cv2.cvtColor(stu_mistake_loc_cont_img, cv2.COLOR_BGR2GRAY)\n",
    "    _, result_img = cv2.threshold(stu_mistake_loc_cont_img, 254, 255, cv2.THRESH_BINARY)\n",
    "    stu_mistake_loc_final = soft_morph_open(result_img)\n",
    "    \n",
    "    return master_contours, student_contours, stu_mistake_loc_final\n",
    "\n",
    "def final_scoring_cross(student_img, master_contours, student_mistake_loc): \n",
    "    mistake_contours, _ = draw_filled_boxes_fetch_contours(student_mistake_loc)\n",
    "    \n",
    "    student_correction = student_img.copy()\n",
    "    student_correction = draw_filled_boxes(master_contours, student_correction)\n",
    "    \n",
    "    mistakes = len(mistake_contours)\n",
    "    total_questions = len(master_contours)\n",
    "    final_score = ((total_questions - mistakes) / total_questions) * 100\n",
    "    final_score = round(final_score, 2)\n",
    "    print(f'total_questions: {total_questions}, mistakes: {mistakes}')\n",
    "    print(f'final score: {final_score}')\n",
    "    \n",
    "    return final_score, student_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = YOLO(\"C:/Users/vian8/Desktop/Tugas2/SNAPGRADE/model/yolo/best.pt\")\n",
    "\n",
    "answer1 = cv2.imread('C:/Users/vian8/Desktop/Tugas2/SNAPGRADE/inputs/cross/student2.jpg')\n",
    "master_sheet = cv2.imread('C:/Users/vian8/Desktop/Tugas2/SNAPGRADE/inputs/cross/master2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial image (1305, 2563, 3) processed to (488, 906)\n",
      "Initial image (1405, 2767, 3) processed to (479, 894)\n",
      "master_key (479, 894) and student_answer (488, 906) uniformed to (479, 894)\n"
     ]
    }
   ],
   "source": [
    "student1 = automatic_warp_transformation(answer1)\n",
    "master_key = automatic_warp_transformation(master_sheet)\n",
    "new_master, new_student = image_uniformization(master_key, student1)\n",
    "\n",
    "# plt.figure(figsize = (15, 10))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(new_master, cmap='gray')\n",
    "# plt.title('Master Key')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(new_student, cmap='gray')\n",
    "# plt.title('Student Answer')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 352x640 45 xs, 78.2ms\n",
      "Speed: 4.9ms preprocess, 78.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 45 xs, 60.3ms\n",
      "Speed: 1.0ms preprocess, 60.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    }
   ],
   "source": [
    "master_box_img, student_box_img = get_cross_answers(model, new_master, new_student)\n",
    "\n",
    "# plt.figure(figsize = (15, 10))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(master_box_img, cmap='gray')\n",
    "# plt.title('Master Key')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(student_box_img, cmap='gray')\n",
    "# plt.title('Student Answer')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Blur: 3860.5569847023316, Final Blur: 73.67991734316864, Kernel Size: 9, Iterations: 3\n",
      "Initial Blur: 2868.2249749690777, Final Blur: 74.44981642952978, Kernel Size: 9, Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "processed_master = core_preprocessing_v2(master_box_img)\n",
    "processed_student = core_preprocessing_v2(student_box_img)\n",
    "\n",
    "# plt.figure(figsize = (15, 10))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(processed_master, cmap='gray')\n",
    "# plt.title('Master Key')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(processed_student, cmap='gray')\n",
    "# plt.title('Student Answer')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "master_contours, student_contours, student_mistake_location = box_contour_handling(processed_master, processed_student)\n",
    "\n",
    "print(len(master_contours))\n",
    "print(len(student_contours))\n",
    "\n",
    "# plt.figure(figsize = (15, 10))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(student_mistake_location, cmap='gray')\n",
    "# plt.title('Student Mistake Location')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_questions: 45, mistakes: 8\n",
      "final score: 82.22\n"
     ]
    }
   ],
   "source": [
    "final_score, student_correction = final_scoring_cross(new_student, master_contours, student_mistake_location)\n",
    "\n",
    "cv2.imshow(\"student's answer vs master key: \", student_correction)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
